{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps for approaching the problem\n",
    "\n",
    "## Pulse database\n",
    "\n",
    "The task that we want the Neural Network to permorm is to *inverse map* the $N\\times N$ real values that represent the SHG-FROG trace to the $2N$ real numbers that represent the real and imaginary parts of the electric field values.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../reports/figs/readme/NN.png\" alt=\"NN scheme for solving the retrieval problem.\" width=\"550\"/><br>\n",
    "  <em>Figure 1: Scheme of the input and output layers of a Neural Network that solves the retrieval problem.</em>\n",
    "</p>\n",
    "\n",
    "Therefore, we will need to generate a database of simulated pulses containing both their SHG-FROG trace and the real and imaginary parts of the pulse electric field. An algorithm to perform such a task can be found at [this link](https://github.com/Loracio/ultrafast-pulse-retrieval/blob/main/src/pulse.hpp#L446) to the [`ultrafast-pulse-retrieval`](https://github.com/Loracio/ultrafast-pulse-retrieval) project, and can be used to create a database as required in this problem, as shown in the file [`testDataBaseGeneration.cpp`](https://github.com/Loracio/ultrafast-pulse-retrieval/blob/main/tests/testDataBaseGeneration.cpp).\n",
    "\n",
    "The generated databases are .csv files containing the following structure:\n",
    "\n",
    "|TBP | $E_{Re \\ 0}$ | $\\dots$ | $E_{Re \\ N-1}$ | $E_{Im \\ 0}$ | $\\dots$ | $E_{Im \\ N-1}$ | $T_{00}$ | $\\dots$ | $T_{N-1 \\ N-1}$\n",
    "\n",
    "where:\n",
    "\n",
    "- TBP: Time bandwidth product\n",
    "- $E_{Re}$: Pulse real part (N columns)\n",
    "- $E_{Im}$: Pulse imaginary part (N columns)\n",
    "- $T_{mn}$: SHG-FROG trace of the pulse (NxN columns)\n",
    "\n",
    "**Tensorflow** will be the library used in this project for the creation and training of Neural Networks, so the database has to be read and processed to be compatible with tensorflow data types.\n",
    "\n",
    "A good practice is to normalize the input and the output of the Neural Network training and validation data. In this case, each trace is normalized by dividing by its maximum value, and the real and imaginary parts of each pulse are divided by the maximum amplitude (polar representation of complex numbers).\n",
    "\n",
    "Database input/output handling functions are implemented inside the [`src/io`](/src/io/) folder, and will read and format the data into training and validation sets to train the Neural Network.\n",
    "\n",
    "Now that we've got the data for training the Neural Network, we have to actually design and train it to solve our problem. We will use a *construction* approach, starting with the simplest possible model and gradually adding features that make it more complex and predictably better at solving the problem.\n",
    "\n",
    "For a better handling and a faster training we will be using pulses with **N=64** for now on. This places limitations on the time-bandwidth product of the pulses we will be able to simulate, and therefore, on the ability of the network to produce pulses of a higher TBP.\n",
    "\n",
    "## Multi Layer Perceptron as a first approach\n",
    "\n",
    "The simplest architecture that can be used to solve the problem is a Multi Layer Perceptron (MLP), which is based on the basic unit of a neural network: the perceptron.\n",
    "\n",
    "The architecture of this network will consist of an input layer with the NxN values of the SHG-FROG trace, one or more hidden layers of a variable number of neurons and an output with the 2N values of the electric field (real and imaginary parts).\n",
    "\n",
    "There are several choices of hyperparameters that must be made to build the neural network:\n",
    "\n",
    "- **Epochs**: number of epochs in which the network will be trained. To avoid overfitting, it is desirable to define an \"early stopping\" so that the training is stopped before the total number of epochs is reached if there is no improvement on the validation set.\n",
    "- **Batch size**: number of training examples used in one iteration of the gradient descent. When the batch size is small, each update to the model parameters is based on fewer examples from the training set. This leads to more noise in the update process, which can help the model escape from local minima but also makes the training process less stable. On the other hand, when the batch size is large, each update is based on more examples, which makes the update process less noisy and more stable, but also more likely to get stuck in local minima. This is closer to the behavior of \"batch\" gradient descent, where each update is based on the entire training set.\n",
    "- **Optimizer**: the optimizer in a neural network determines how the model updates its parameters in response to the calculated error gradient. The error gradient, which is computed using backpropagation, indicates the direction in which the model parameters need to be adjusted to minimize the loss function.\n",
    "- **Learning rate**: the size of the steps taken to reach the minimum of the loss function. If the learning rate is too large, it may overshoot the optimal point. If it's too small, training will take too long.\n",
    "- **Loss**: The function that the model tries to minimize during training. It's a measure of how far the model's predictions are from the true values.\n",
    "- **Training and validation metrics**: metrics used to measure the performance of the model on the training and validation datasets.\n",
    "- **Number of hidden layers**: number of layers in the neural network that are neither the input nor the output layer.\n",
    "- **Number of neurons per hidden layer**: number of neurons in each hidden layer.\n",
    "- **Activation function**: the function used to transform the input signal of a neuron in the network to its output signal.\n",
    "- **Initializer**: the initializer of weights and biases in a neural network is a method or function that sets the initial values of the model's parameters (weights and biases) before training starts. The choice of initialization can significantly affect the speed of convergence and the final performance of the model.\n",
    "- **Dropout**: regularization technique where randomly selected neurons are ignored during training. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. It helps prevent overfitting.\n",
    "\n",
    "Deciding which of these parameters to use often comes down to a process of trial and error, although it is possible to choose them with \"a bit of common sense\".\n",
    "\n",
    "Let's get started with the coding. To record the neural network trainings we will use the [Weights & Biases (wandb)](https://wandb.ai/site) tool. This will allow us to have in a clean and orderly way each of the networks that we are going to train in different projects, where we will be able to see the different graphs of metrics and network hyperparameters used.\n",
    "\n",
    "If you want to use the script version of this notebook, check [`/tests/test_launch_MLP.py`](/tests/test_launch_MLP.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 00:54:24.167573: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-09 00:54:24.190776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# We start with importing the modules we'll need\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "import wandb # Weights and Biases\n",
    "from wandb.keras import WandbCallback # We'll use this to log our metrics to Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we will use data with N=64 samples. We generate a database containing 2500 pulses and create a variable pointing to its path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 64\n",
    "NUMBER_OF_PULSES = 1000\n",
    "FILE_PATH = f\"../data/generated/N{N}/{NUMBER_OF_PULSES}_randomPulses_N{N}.csv\"\n",
    "# Handle error if path does not exist\n",
    "try:\n",
    "    with open(FILE_PATH) as f:\n",
    "        pass\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please generate the pulse database first.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wandb uses a dictionary to store the model's parameters. We can store whatever we want and think its relevant to reproduce the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epochs': 50,\n",
    "    'batch_size': 256,\n",
    "    'log_step': 50,\n",
    "    'val_log_step': 50,\n",
    "    'optimizer': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'loss': 'mse',\n",
    "    'train_metrics': 'MeanSquaredError',\n",
    "    'val_metrics': 'MeanSquaredError',\n",
    "    'n_hidden_layers': 2,\n",
    "    'n_neurons_per_layer': 512,\n",
    "    'activation': 'relu',\n",
    "    'dropout': 0.1,\n",
    "    'patience': 10,\n",
    "    'training_size': 0.8,\n",
    "    'database': f'{NUMBER_OF_PULSES}_randomPulses_N{N}',\n",
    "    'arquitecture': 'MLP',\n",
    "    'input_shape': (N, N),\n",
    "    'output_shape': (int(2 * N)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function `load_and_norm_data` that returns a `tf.data.Dataset` object with the normalized pulse database read from file. All traces are normalized by their maximum value. The real part and imaginary part of the pulse are normalized by dividing by the maximum absolute value (module) of the complex number. Note that we also have the TBP of the pulses in the first column of the db. \n",
    "\n",
    "Then, defining the function `process_data` the dataset is processed. Data is batched and shuffled, dividing it into train and test sets. The output are two datasets containing the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_norm_data(FILE_PATH, N, NUMBER_OF_PULSES):\n",
    "    \"\"\"\n",
    "    This function preprocesses the data from the database, iterating over it.\n",
    "    It returns a tensorflow dataset.\n",
    "\n",
    "    Note that we also have the TBP of the pulses in the first column of the db.\n",
    "    We want to save them in a separate array, so we can use them later.\n",
    "\n",
    "    The function also normalizes each trace, by dividing by the maximum value.\n",
    "    The real part and imaginary part of the pulse are normalized by dividing by\n",
    "    the maximum absolute value (module) of the complex number.\n",
    "\n",
    "    Args:\n",
    "        FILE_PATH: str\n",
    "            Path to the database file\n",
    "        N: int\n",
    "            Number of points in the SHG-FROG trace\n",
    "        NUMBER_OF_PULSES: int\n",
    "            Number of pulses in the database\n",
    "\n",
    "    Returns:\n",
    "        dataset: tf.data.Dataset\n",
    "            Dataset with the pulse database\n",
    "    \"\"\"\n",
    "    # Create a record_defaults with 1 + 2N + N*N elements that are tf.float32\n",
    "    db_record_defaults = [tf.float32] * (1 + 2*N + N*N)\n",
    "\n",
    "    # Read the database\n",
    "    pulse_db = tf.data.experimental.CsvDataset(\n",
    "        FILE_PATH, record_defaults=db_record_defaults, header=False)\n",
    "\n",
    "    # Create empty datasets\n",
    "    tbp_dataset = tf.data.Dataset.from_tensor_slices([])\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices([])\n",
    "    target_dataset = tf.data.Dataset.from_tensor_slices([])\n",
    "\n",
    "    # Iterate over the database\n",
    "    for i, pulse in enumerate(pulse_db):\n",
    "        # Save the TBP in the tbp_dataset\n",
    "        tbp_dataset = tbp_dataset.concatenate(\n",
    "            tf.data.Dataset.from_tensor_slices(tf.reshape(pulse[0], (1,))))\n",
    "\n",
    "        # Save the SHG-FROG trace in the train_dataset and normalize\n",
    "        shg_frog_trace = tf.reshape(pulse[2*N + 1:], (1, N, N))\n",
    "        normalized_trace = shg_frog_trace / tf.reduce_max(tf.abs(shg_frog_trace))\n",
    "        train_dataset = train_dataset.concatenate(\n",
    "            tf.data.Dataset.from_tensor_slices(normalized_trace))\n",
    "\n",
    "        # Save the pulse in the target_dataset and normalize\n",
    "        pulse_real = tf.reshape(pulse[1:N + 1], (1, N))\n",
    "        pulse_imag = tf.reshape(pulse[N + 1:2*N + 1], (1, N))\n",
    "\n",
    "        # Combine real and imaginary parts into complex numbers\n",
    "        pulse_complex = tf.complex(pulse_real, pulse_imag)\n",
    "\n",
    "        # Find the maximum absolute value (module) of the complex numbers\n",
    "        max_module = tf.reduce_max(tf.abs(pulse_complex))\n",
    "\n",
    "        # Normalize the real and imaginary parts by the maximum module\n",
    "        normalize_pulse_real = pulse_real / max_module\n",
    "        normalize_pulse_imag = pulse_imag / max_module\n",
    "\n",
    "        normalized_pulse = tf.concat([normalize_pulse_real, normalize_pulse_imag], axis=1)\n",
    "        target_dataset = target_dataset.concatenate(\n",
    "            tf.data.Dataset.from_tensor_slices(normalized_pulse))\n",
    "\n",
    "    # Create the final dataset\n",
    "    dataset = tf.data.Dataset.zip((tbp_dataset, train_dataset, target_dataset))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(N, NUMBER_OF_PULSES, pulse_dataset, training_size, BATCH_SIZE, SHUFFLE_BUFFER_SIZE=None):\n",
    "    \"\"\"\n",
    "    In this function the data is processed.\n",
    "    Data is batched and shuffled, dividing it into train and test sets.\n",
    "\n",
    "    Args:\n",
    "        N (int): Number of time steps\n",
    "        NUMBER_OF_PULSES (int): Number of pulses in the database\n",
    "        pulse_dataset (tf.data.Dataset): Dataset containing the pulses\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (tf.data.Dataset): Dataset containing the training pulses\n",
    "        test_dataset (tf.data.Dataset): Dataset containing the test pulses\n",
    "    \"\"\"\n",
    "    if SHUFFLE_BUFFER_SIZE is None:\n",
    "        SHUFFLE_BUFFER_SIZE = NUMBER_OF_PULSES\n",
    "\n",
    "    # Select the y and z data from pulse dataset, which contain the SHG-FROG trace and the electric field of the pulse in the time domain\n",
    "    pulse_dataset = pulse_dataset.map(lambda x, y, z: (y, z))\n",
    "\n",
    "    # Split the dataset into train and test, shuffle and batch the train dataset\n",
    "    train_dataset = pulse_dataset.take(int(training_size * NUMBER_OF_PULSES)).shuffle(buffer_size=SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_dataset = pulse_dataset.skip(int(training_size * NUMBER_OF_PULSES)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the functions\n",
    "pulse_dataset = load_and_norm_data(FILE_PATH, N, NUMBER_OF_PULSES)\n",
    "train_dataset, test_dataset = process_data(N, NUMBER_OF_PULSES, pulse_dataset,\n",
    "                                            config['training_size'], config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvloras\u001b[0m (\u001b[33mpulse-retrieval-with-nn\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vloras/Desktop/github/NN-for-pulse-retrieval/notebooks/wandb/run-20240209_005451-9e6rvrvn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pulse-retrieval-with-nn/MLP_example/runs/9e6rvrvn' target=\"_blank\">MLP test run #1</a></strong> to <a href='https://wandb.ai/pulse-retrieval-with-nn/MLP_example' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pulse-retrieval-with-nn/MLP_example' target=\"_blank\">https://wandb.ai/pulse-retrieval-with-nn/MLP_example</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pulse-retrieval-with-nn/MLP_example/runs/9e6rvrvn' target=\"_blank\">https://wandb.ai/pulse-retrieval-with-nn/MLP_example/runs/9e6rvrvn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb for the tracking. \n",
    "# We need to pass the configuration, the project name and name of the run\n",
    "run = wandb.init(project=\"MLP_example\", config=config,\n",
    "                    name='MLP test run #1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that creates a Multi Layer Perceptron based on the values of the config dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP(input_shape, output_shape, n_hidden_layers, n_neurons_per_layer, activation, dropout=None):\n",
    "    \"\"\"\n",
    "    This function creates a MLP model with the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input layer\n",
    "        output_shape (tuple): Shape of the output layer\n",
    "        n_hidden_layers (int): Number of hidden layers\n",
    "        n_neurons_per_layer (int): Number of neurons per hidden layer\n",
    "        activation (str): Activation function to use in hidden layers\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape, name=\"input\")\n",
    "    flatten_layer = keras.layers.Flatten()(inputs)\n",
    "    # Add the hidden layers given by the arguments\n",
    "    dense_layer = keras.layers.Dense(n_neurons_per_layer, activation=activation)(flatten_layer)\n",
    "    # Only add dropout if dropout is not None\n",
    "    if dropout is not None:\n",
    "            dense_layer = keras.layers.Dropout(dropout)(dense_layer)\n",
    "    for i in range(n_hidden_layers - 1):\n",
    "        dense_layer = keras.layers.Dense(n_neurons_per_layer, activation=activation)(dense_layer)\n",
    "        # Only add dropout if dropout is not None\n",
    "        if dropout is not None:\n",
    "            dense_layer = keras.layers.Dropout(dropout)(dense_layer)\n",
    "    outputs = keras.layers.Dense(output_shape, name=\"output\")(dense_layer)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 64, 64)]          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 128)               65664     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2425984 (9.25 MB)\n",
      "Trainable params: 2425984 (9.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the config\n",
    "model = MLP(config['input_shape'], config['output_shape'], config['n_hidden_layers'],\n",
    "            config['n_neurons_per_layer'], config['activation'], config['dropout'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now to define the training step of the NN. It will consist of a training step with the training dataset and a validation step with the validation dataset. Additionally, the metrics will be stored with wandb after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_MLP(x, y, model, optimizer, loss_fn, train_acc_metric):\n",
    "    \"\"\"\n",
    "    Example training step for a MLP model.\n",
    "\n",
    "    Args:\n",
    "        x (tf.Tensor): Input data\n",
    "        y (tf.Tensor): Target data\n",
    "        model (tf.keras.Model): Model to train\n",
    "        optimizer (tf.keras.optimizers.Optimizer): Optimizer to use\n",
    "        loss_fn (tf.keras.losses.Loss): Loss function to use\n",
    "        train_acc_metric (tf.keras.metrics.Metric): Metric to use for training accuracy\n",
    "\n",
    "    Returns:\n",
    "        loss_value (tf.Tensor): Loss value for the training step\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        results = model(x, training=True)\n",
    "        loss_value = loss_fn(y, results)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "    train_acc_metric.update_state(y, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step_MLP(x, y, model, loss_fn, test_acc_metric):\n",
    "    \"\"\"\n",
    "    Example test step for a MLP model.\n",
    "\n",
    "    Args:\n",
    "        x (tf.Tensor): Input data\n",
    "        y (tf.Tensor): Target data\n",
    "        model (tf.keras.Model): Model to train\n",
    "        loss_fn (tf.keras.losses.Loss): Loss function to use\n",
    "        test_acc_metric (tf.keras.metrics.Metric): Metric to use for test accuracy\n",
    "\n",
    "    Returns:\n",
    "        loss_value (tf.Tensor): Loss value for the validation step\n",
    "    \"\"\"\n",
    "    val_results = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_results)\n",
    "\n",
    "    test_acc_metric.update_state(y, val_results)\n",
    "\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(train_dataset, test_dataset, model, optimizer, loss_fn, train_acc_metric, test_acc_metric, epochs, log_step, val_log_step, patience):\n",
    "    \"\"\"\n",
    "    Trainin step for a MLP model. Updates the weights of the model using the gradients computed by the loss function.\n",
    "    Saves the training and validation loss and accuracy in wandb.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (tf.data.Dataset): Training dataset.\n",
    "        test_dataset (tf.data.Dataset): Test dataset.\n",
    "        model (tf.keras.Model): Model to train.\n",
    "        optimizer (tf.keras.optimizers): Optimizer to use.\n",
    "        loss_fn (tf.keras.losses): Loss function to use.\n",
    "        train_acc_metric (tf.keras.metrics): Training accuracy metric.\n",
    "        test_acc_metric (tf.keras.metrics): Test accuracy metric.\n",
    "        epochs (int): Number of epochs to train.\n",
    "        log_step (int): Number of steps to log training metrics.\n",
    "        val_log_step (int): Number of steps to log validation metrics.\n",
    "        patience (int): Number of epochs to wait for improvement in validation loss before early stopping.\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            loss_value = train_step_MLP(x_batch_train, y_batch_train,\n",
    "                                    model, optimizer,\n",
    "                                    loss_fn, train_acc_metric)\n",
    "            average_loss_value = tf.reduce_mean(loss_value)\n",
    "            train_loss.append(float(average_loss_value))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for step, (x_batch_val, y_batch_val) in enumerate(test_dataset):\n",
    "            val_loss_value = test_step_MLP(x_batch_val, y_batch_val,\n",
    "                                       model, loss_fn,\n",
    "                                       test_acc_metric)\n",
    "            average_loss_value = tf.reduce_mean(val_loss_value)\n",
    "            val_loss.append(float(average_loss_value))\n",
    "\n",
    "        avg_val_loss = np.mean(val_loss)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter > patience:\n",
    "            print(\"Early stopping due to no improvement in validation loss\")\n",
    "            break\n",
    "\n",
    "        # Display metrics at the end of each epoch\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        test_acc = test_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(test_acc),))\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_acc_metric.reset_states()\n",
    "        test_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "        # log metrics using wandb.log\n",
    "        wandb.log({'epochs': epoch,\n",
    "                   'train_loss': np.mean(train_loss),\n",
    "                   'train_acc': float(train_acc),\n",
    "                   'test_loss': np.mean(val_loss),\n",
    "                   'test_acc': float(test_acc)\n",
    "                   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to initialize the optimizer with the selected learning rate and call the `train_MLP` function to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model with the config\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m train_MLP(train_dataset, test_dataset, model,\n\u001b[1;32m      8\u001b[0m             epochs\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m             optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     10\u001b[0m             loss_fn\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mget(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     11\u001b[0m             train_acc_metric\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mget(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     12\u001b[0m             test_acc_metric\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mget(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     13\u001b[0m             log_step\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_step\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m             val_log_step\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_log_step\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     15\u001b[0m             patience\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m             )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Finish the run in wandb\u001b[39;00m\n\u001b[1;32m     19\u001b[0m run\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m, in \u001b[0;36mtrain_MLP\u001b[0;34m(train_dataset, test_dataset, model, optimizer, loss_fn, train_acc_metric, test_acc_metric, epochs, log_step, val_log_step, patience)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (x_batch_train, y_batch_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataset):\n\u001b[1;32m     30\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m train_step_MLP(x_batch_train, y_batch_train,\n\u001b[1;32m     31\u001b[0m                             model, optimizer,\n\u001b[1;32m     32\u001b[0m                             loss_fn, train_acc_metric)\n\u001b[0;32m---> 33\u001b[0m     average_loss_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(loss_value)\n\u001b[1;32m     34\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(average_loss_value))\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Run a validation loop at the end of each epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/defenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/defenv/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "# Set the optimizer with the config with its learning rate\n",
    "optimizer = keras.optimizers.get(config['optimizer'])\n",
    "optimizer.learning_rate = config['learning_rate']\n",
    "\n",
    "\n",
    "# Train the model with the config\n",
    "train_MLP(train_dataset, test_dataset, model,\n",
    "            epochs=config['epochs'],\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=keras.losses.get(config['loss']),\n",
    "            train_acc_metric=keras.metrics.get(config['train_metrics']),\n",
    "            test_acc_metric=keras.metrics.get(config['val_metrics']),\n",
    "            log_step=config['log_step'],\n",
    "            val_log_step=config['val_log_step'],\n",
    "            patience=config['patience']\n",
    "            )\n",
    "\n",
    "# Finish the run in wandb\n",
    "run.finish()\n",
    "\n",
    "# Save the model\n",
    "model.save(f\"./trained_models/FCNN/{config['arquitecture']}_test.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why, but this doesn't work here on the Jupyter Notebook but it works if used in a normal Python script.\n",
    "\n",
    "Anyways, now the results of the training can be viewed on the Weights & Biases webpage.\n",
    "\n",
    "For taking a look at the results of the network, we can use the GUI class defined in [`/src/visualization/visualization.py`](/src/visualization/visualization.py) to view the results.\n",
    "\n",
    "Check an example in [`/tests/test_visualization.py`](/tests/test_visualization.py)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
